<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>集群智能与群体行为 - arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>集群智能与群体行为 - arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-18 13:10</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251118_1310</div>
    <div class="row"><div class="card">
<div class="title">Instruction Tuning Chronologically Consistent Language Models</div>
<div class="meta-line">Authors: Songrun He, Linying Lv, Asaf Manela, Jimmy Wu</div>
<div class="meta-line">First: 2025-10-13T17:45:24+00:00 · Latest: 2025-11-17T18:56:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11677v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11677v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a family of chronologically consistent, instruction-tuned large language models to eliminate lookahead bias. Each model is trained only on data available before a clearly defined knowledge-cutoff date, ensuring strict temporal separation from any post-cutoff data. The resulting framework offers (i) a simple, conversational chat interface, (ii) fully open, fixed model weights that guarantee replicability, and (iii) a conservative lower bound on forecast accuracy, isolating the share of predictability that survives once training leakage is removed. Together, these features provide researchers with an easy-to-use generative AI tool useful for a wide range of prediction tasks that is free of lookahead bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>按时间顺序一致的语言模型指令调优</div>
<div class="mono" style="margin-top:8px">我们引入了一系列具有时间一致性、经过指令微调的大型语言模型，以消除前瞻偏差。每个模型仅使用明确定义的知识截止日期之前的数据进行训练，确保与截止后数据的严格时间隔离。该框架具备：(i) 简洁的对话式聊天界面，(ii) 完全开放且固定的模型权重以确保可复现性，(iii) 预测准确性的保守下限——在消除训练数据泄露后，仍能保留可预测性部分。这些特性共同为研究人员提供了一个易于使用的生成式AI工具，适用于各类无前瞻偏差的预测任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to eliminate lookahead bias in large language models by developing chronologically consistent, instruction-tuned models trained exclusively on data available before specific cutoff dates. The method ensures strict temporal separation from post-cutoff information through instruction tuning with clearly defined knowledge boundaries. Experimental results demonstrate that these models provide a conservative lower bound on forecast accuracy, isolating genuine predictability while offering fully open, replicable model weights and a conversational interface suitable for various prediction tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过开发时间一致的大型语言模型来消除前瞻性偏差，这些模型仅使用特定知识截止日期前的数据进行训练，确保不接触未来信息。方法采用指令微调构建模型系列，提供对话式聊天界面、开放固定权重以保证可复现性，以及保守的预测精度基准。实验结果表明，这些模型通过消除训练数据泄露，有效隔离了任务中的真实可预测性，为无偏预测应用提供了可靠工具。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting</div>
<div class="meta-line">Authors: Jiangnan Ye, Jiedong Zhuang, Lianrui Mu, Wenjie Zheng, Jiaqi Hu, Xingze Zou, Jing Wang, Haoji Hu</div>
<div class="meta-line">First: 2025-11-17T18:37:41+00:00 · Latest: 2025-11-17T18:37:41+00:00</div>
<div class="meta-line">Comments: Submitting for Neurocomputing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13684v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需训练的多视角IC-Light扩展：面向文本位置感知的场景重光照</div>
<div class="mono" style="margin-top:8px">本文提出GS-Light——一种基于高斯溅射(3DGS)三维场景表示的高效文本位置感知流程，用于文本引导的场景重光照。该方法无需训练即可将单输入扩散模型扩展至多视角输入。通过大型视觉语言模型解析用户提示（可能包含光照方向、颜色、强度或参考对象），结合几何语义估计器（深度、表面法线、语义分割）将光照先验与视角几何约束融合，计算光照图并生成各视角初始潜码。这些精细推导的初始潜码引导扩散模型生成更精准符合用户预期的重光照结果，尤其在光照方向控制方面。通过将多视角渲染图像与初始潜码输入多视角重光照模型，最终生成高保真艺术化重光照图像，并对3DGS场景进行外观微调获得完整重光照三维场景。在室内外场景的评测中，GS-Light在定量指标（多视角一致性、成像质量、美学评分、语义相似度等）与定性评估（用户研究）上均优于当前先进基线方法（包括逐视角重光照、视频重光照及场景编辑方法）。代码与资源将在发表时开放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving text-guided relighting in 3D Gaussian Splatting scenes with accurate lighting direction control. The method, GS-Light, extends a single-input diffusion model to multi-view inputs without training, using a large vision-language model to parse text prompts into lighting priors and fusing them with geometric constraints to generate initial latent codes for guiding the diffusion process. Experimental results on indoor and outdoor scenes show that GS-Light outperforms state-of-the-art baselines in multi-view consistency, image quality, and user satisfaction, as validated by quantitative metrics and user studies.</div>
<div class="mono" style="margin-top:8px">本研究针对文本引导的3D场景重光照问题，提出了GS-Light这一无需训练的方法，将单输入扩散模型扩展至多视图输入。该方法利用大型视觉语言模型解析用户提示生成光照先验，结合深度、法线和分割图提供的几何与语义约束计算光照图和初始潜码，通过多视图扩散模型生成重光照图像后对高斯泼溅场景进行微调。在室内外场景的实验中，GS-Light在多视图一致性、图像质量、美学评分和语义相似度等定量指标及用户研究中均优于现有先进基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Arcee: Differentiable Recurrent State Chain for Generative Vision Modeling with Mamba SSMs</div>
<div class="meta-line">Authors: Jitesh Chavan, Rohit Lal, Anand Kamat, Mengjia Xu</div>
<div class="meta-line">First: 2025-11-14T12:44:02+00:00 · Latest: 2025-11-17T18:00:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11243v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11243v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-space models (SSMs), Mamba in particular, are increasingly adopted for long-context sequence modeling, providing linear-time aggregation via an input-dependent, causal selective-scan operation. Along this line, recent &quot;Mamba-for-vision&quot; variants largely explore multiple scan orders to relax strict causality for non-sequential signals (e.g., images). Rather than preserving cross-block memory, the conventional formulation of the selective-scan operation in Mamba reinitializes each block&#x27;s state-space dynamics from zero, discarding the terminal state-space representation (SSR) from the previous block. Arcee, a cross-block recurrent state chain, reuses each block&#x27;s terminal state-space representation as the initial condition for the next block. Handoff across blocks is constructed as a differentiable boundary map whose Jacobian enables end-to-end gradient flow across terminal boundaries. Key to practicality, Arcee is compatible with all prior &quot;vision-mamba&quot; variants, parameter-free, and incurs constant, negligible cost. As a modeling perspective, we view terminal SSR as a mild directional prior induced by a causal pass over the input, rather than an estimator of the non-sequential signal itself. To quantify the impact, for unconditional generation on CelebA-HQ (256$\times$256) with Flow Matching, Arcee reduces FID$\downarrow$ from $82.81$ to $15.33$ ($5.4\times$ lower) on a single scan-order Zigzag Mamba baseline. Efficient CUDA kernels and training code will be released to support rigorous and reproducible research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Arcee：基于Mamba状态空间模型的可微分循环状态链生成视觉建模</div>
<div class="mono" style="margin-top:8px">状态空间模型（特别是Mamba）正日益广泛应用于长上下文序列建模，通过输入依赖的因果选择性扫描操作实现线性时间聚合。在此背景下，近期“视觉Mamba”变体主要探索多种扫描顺序以放松非序列信号（如图像）的严格因果性。与传统Mamba中每个块从零初始化状态空间动态、丢弃前一块终止状态空间表示的做法不同，Arcee通过跨块循环状态链重用每个块的终止状态空间表示作为下一块的初始条件。块间传递被构建为可微分边界映射，其雅可比矩阵支持跨终止边界的端到端梯度流。Arcee的关键实用优势在于：兼容所有现有视觉Mamba变体、无需参数引入、仅产生恒定且可忽略的计算开销。从建模视角，我们将终止状态空间表示视为因果遍历输入产生的温和方向先验，而非非序列信号本身的估计量。在CelebA-HQ（256×256）数据集上的无条件流匹配生成实验中，Arcee将单次Zigzag扫描基线模型的FID从82.81降至15.33（降低5.4倍）。我们将发布高效CUDA内核与训练代码以支持严谨可复现的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation in Mamba state-space models where the selective-scan operation reinitializes state-space dynamics for each block, discarding valuable terminal state representations from previous blocks. The proposed Arcee method introduces a differentiable recurrent state chain that reuses each block&#x27;s terminal state-space representation as the initial condition for the next block, enabling end-to-end gradient flow across block boundaries through a differentiable boundary map. Experimental results on unconditional generation using CelebA-HQ 256×256 images with Flow Matching demonstrate that Arcee achieves a 5.4× improvement, reducing FID from 82.81 to 15.33 compared to the baseline Zigzag Mamba model.</div>
<div class="mono" style="margin-top:8px">本研究针对Mamba状态空间模型中选择性扫描操作在每个块重新初始化状态空间动态、丢弃先前块终端状态表示的问题，提出Arcee方法构建可微分的循环状态链，将每个块的终端状态空间表示作为下一块的初始条件，通过可微分边界映射实现跨块端到端梯度流动。在CelebA-HQ 256×256图像无条件生成实验中，采用流匹配方法，Arcee将FID从82.81显著降低至15.33，相比基线Zigzag Mamba模型实现了5.4倍的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding</div>
<div class="meta-line">Authors: Shrenik Patel, Daivik Patel</div>
<div class="meta-line">First: 2025-11-17T17:56:14+00:00 · Latest: 2025-11-17T17:56:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13644v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one&#x27;s keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block&#x27;s full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CacheFlow：用于高效长视频理解的压缩流式内存</div>
<div class="mono" style="margin-top:8px">长视频问答任务使当前视觉语言模型不堪重负，因为注意力机制与键值缓存会随运行时长增长，导致要么推理成本高昂，要么只能采用短视的滑动窗口方法。我们提出CacheFlow这一免训练流程，将动态令牌丢弃与压缩式长期记忆相结合：动态令牌丢弃通过余弦相似度在线修剪每帧图像块令牌，存活令牌被打包为固定尺寸块。这种逐帧在线处理特性使本方法天然适配流式视频问答场景。在处理数据块时，通过微型循环编码器构建检索索引，完整键值对则卸载存储并在生成时重新加载以保持答案保真度。推理阶段采用基于共识的检索机制，仅召回最相关的Top-K数据块，同时关注检索上下文与本地上下文以实现精准长程推理。CacheFlow具备即插即用、架构无关、无需微调的特性。离线与流式视频问答基准测试表明，该方法在减少最高87%令牌处理量的同时优于现有基线。这种双重技术路径使视觉语言模型兼具高效性与上下文感知能力，为实用化长视频理解铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Long-form video question answering challenges current vision-language models due to the computational burden of growing attention and key-value caches during runtime. CacheFlow addresses this with a training-free pipeline combining Dynamic Token Dropping, which prunes redundant tokens per frame via cosine similarity, and a compressive long-term memory that summarizes and retrieves key blocks for generation. Experimental results on streaming and offline VQA benchmarks show that CacheFlow outperforms strong baselines while reducing token processing by up to 87%, enabling efficient and context-aware long-form video understanding.</div>
<div class="mono" style="margin-top:8px">长视频问答任务对现有视觉语言模型构成挑战，因为运行时注意力与键值缓存会持续增长，导致计算开销巨大。CacheFlow提出了一种无需训练的方法，结合动态令牌丢弃和压缩式长期记忆机制，前者通过余弦相似度在线剪裁冗余令牌，后者则对关键块进行摘要与检索以支持生成。实验表明，在多个视频问答基准测试中，CacheFlow优于现有基线方法，同时减少了高达87%的令牌处理量，实现了高效且具备长上下文感知能力的视频理解。</div>
</details>
</div>
<div class="card">
<div class="title">CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product</div>
<div class="meta-line">Authors: Kaiwen Xue, Chenglong Li, Zhonghong Ou, Guoxin Zhang, Kaoyan Lu, Shuai Lyu, Yifan Zhu, Ping Zong Junpeng Ding, Xinyu Liu, Qunlin Chen, Weiwei Qin, Yiran Shen, Jiayi Cen</div>
<div class="meta-line">Venue: AAAI poster</div>
<div class="meta-line">First: 2025-11-17T17:34:05+00:00 · Latest: 2025-11-17T17:34:05+00:00</div>
<div class="meta-line">Comments: 13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13626v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CreBench：从创意到过程再到产出的类人创造力评估</div>
<div class="mono" style="margin-top:8px">人类定义的创造力具有高度抽象性，这使多模态大语言模型在理解和评估符合人类判断的创造力方面面临挑战。现有基准的缺失进一步加剧了这一困境。为此，我们提出CreBench，其包含两个核心组件：1）覆盖从创意构思到执行过程再到最终产出的多维度评估基准；2）CreBench多模态指令调优数据集——包含2.2K个多源多模态数据、79.2K条人类反馈及470万条多类型指令。特别地，我们通过GPT优化人类反馈以激活更强的创造力评估能力，确保模型能处理多样化创造力相关查询。CreBench为构建理解人类对齐创造力的多模态大模型奠定基础。基于该基准，我们微调开源通用多模态大模型，最终形成创造力评估专家模型CreExpert。大量实验表明，相较于包括最先进的GPT-4V和Gemini-Pro-Vision在内的前沿模型，所提出的CreExpert模型在人类创造力评估对齐方面实现显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of aligning multimodal large language models (MLLMs) with human-defined creativity, which is abstract and lacks standardized benchmarks. The authors introduce CreBench, comprising a multidimensional creativity evaluation benchmark and CreMIT—a dataset with 2.2K multimodal data points, 79.2K human feedback entries, and 4.7M instructions refined using GPT to enhance creativity assessment. Experimental results show that fine-tuned models, termed CreExpert, achieve significantly better alignment with human creativity evaluations than state-of-the-art MLLMs like GPT-4V and Gemini-Pro-Vision.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态大语言模型难以与人类定义的抽象创造力评估对齐且缺乏基准的问题，提出了CreBench，包含多维评估框架和CreMIT数据集，后者整合了2.2K多模态数据、79.2K人类反馈及4.7M指令，并利用GPT优化反馈以强化创造力评估能力。实验表明，基于该基准微调的开源模型CreExpert在创造力评估上与人类判断的一致性显著优于包括GPT-4V和Gemini-Pro-Vision在内的先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation</div>
<div class="meta-line">Authors: Quoc-Huy Trinh, Mustapha Abdullahi, Do Duy Hung Trinh, Bo Zhao, Debesh Jha</div>
<div class="meta-line">First: 2025-11-14T11:21:48+00:00 · Latest: 2025-11-17T17:08:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11177v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11177v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Viper-F1：基于跨模态状态空间调制的快速细粒度多模态理解</div>
<div class="mono" style="margin-top:8px">多模态大语言模型近期在视觉语言理解领域取得显著进展，但其高昂计算成本限制了在机器人操控、个人助手和智能相机等资源受限场景的部署。现有方法多依赖基于Transformer的交叉注意力机制，其二次计算复杂度制约了效率。此外，小型视觉语言模型往往难以精准捕捉任务相关的细粒度视觉区域，导致细粒度推理任务性能下降，影响实际应用效果。为解决这些问题，我们提出Viper-F1——一种采用液态状态空间动力学替代注意力机制的混合状态空间视觉语言模型。为增强视觉定位能力，我们提出词元-网格关联模块，通过计算文本词元与图像块间的轻量级关联，并借助FiLM条件机制调控状态空间动力学，使模型能选择性聚焦与文本提示相关的视觉区域，同时保持线性时间推理复杂度。多基准测试结果表明，Viper-F1在显著提升效率的同时实现了精准的细粒度理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational cost and limited fine-grained visual grounding of existing multimodal large language models, this work introduces Viper-F1, a hybrid state-space vision-language model that replaces Transformer cross-attention with efficient liquid state-space dynamics. The method incorporates a token-grid correlation module to compute lightweight correlations between text tokens and image patches, modulating state-space dynamics via FiLM conditioning to selectively emphasize task-relevant visual regions while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate fine-grained understanding with significantly improved efficiency.</div>
<div class="mono" style="margin-top:8px">针对现有多模态大语言模型计算成本高、细粒度视觉定位能力不足的问题，本研究提出Viper-F1混合状态空间视觉语言模型，用高效液态状态空间动态替代Transformer交叉注意力机制。该方法通过词元-网格关联模块计算文本词元与图像块间的轻量级关联，并利用FiLM条件调制状态空间动态，从而在保持线性时间推理的同时选择性聚焦任务相关视觉区域。多基准测试结果表明，Viper-F1在显著提升效率的同时实现了精确的细粒度视觉理解。</div>
</details>
</div>
<div class="card">
<div class="title">PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</div>
<div class="meta-line">Authors: Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</div>
<div class="meta-line">First: 2025-08-14T10:03:47+00:00 · Latest: 2025-11-17T16:36:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10501v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.10501v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PASS：面向可解释与自适应胸部X光推理的概率性智能体超网采样</div>
<div class="mono" style="margin-top:8px">现有工具增强型智能体系统在现实应用中存在三大局限：(i) 黑箱推理步骤削弱决策可信度并引发安全风险；(ii) 对医疗任务至关重要的多模态融合能力不足；(iii) 僵化且计算低效的智能体流程。我们提出PASS（概率性智能体超网采样）——首个针对胸部X光推理的多模态解决方案。该框架通过多工具图自适应采样智能工作流，生成带可解释概率标注的决策路径。面对多模态医疗数据构成的复杂CXR推理任务，PASS利用其习得的任务条件分布，在超网各层动态选择最优工具，提供概率标注轨迹以供事后审计，直接增强医疗AI安全性。该系统持续将关键发现压缩至动态演进的个性化记忆库，并智能决策是否深化推理或提前终止以提升效率。为平衡性能与成本的帕累托边界，我们设计了三阶段训练策略：专家知识预热、对比路径排序和成本感知强化学习。同时构建CAB-E基准测试平台，用于评估安全关键型多步骤自由式CXR推理。多基准实验表明，PASS在准确率、AUC、LLM-J等指标上显著超越基线模型，在控制计算成本的同时推动可解释、自适应、多模态医疗智能体系统的范式革新。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing tool-augmented agentic systems face limitations in real-world healthcare applications due to opaque reasoning, weak multimodal integration, and inefficient pipelines. To address these, the authors propose PASS, a multimodal framework that adaptively samples agentic workflows over a multi-tool graph, generating interpretable probability-annotated decision paths and dynamically managing reasoning depth for efficiency via a three-stage training procedure. Experimental results on comprehensive benchmarks demonstrate that PASS significantly outperforms baselines in accuracy, AUC, and other metrics while effectively balancing computational costs.</div>
<div class="mono" style="margin-top:8px">现有工具增强的智能体系统在现实医疗应用中存在黑盒推理、多模态整合不足和流程僵化等问题。为此，本研究提出PASS多模态框架，通过在多工具图上自适应采样智能体工作流，利用学习到的任务条件分布逐层选择工具，并生成带概率标注的决策路径以提升可解释性。该框架采用包含专家知识预热、对比路径排序和成本感知强化学习的三阶段训练方法，以优化性能与成本的平衡。在CAB-E等基准测试上的实验结果表明，PASS在准确率、AUC和LLM-J等多项指标上显著优于基线方法，同时有效控制了计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint</div>
<div class="meta-line">Authors: Suqing Wang, Ziyang Ma, Xinyi Li, Zuchao Li</div>
<div class="meta-line">Venue: AAAI 2026 Oral</div>
<div class="meta-line">First: 2025-11-09T13:57:59+00:00 · Latest: 2025-11-17T16:20:58+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 (Oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06390v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06390v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Transformer中的幽灵：基于SVD指纹的大语言模型谱系追溯</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）发展迅猛且在各领域广泛应用。由于从头训练需要巨大计算成本和数据需求，许多开发者选择对现有开源模型进行微调或修改。虽然多数遵守开源协议，但部分模型在明显衍生自公开模型的情况下仍虚假宣称原创训练，这引发了对知识产权保护的迫切关注，并凸显了验证模型来源可靠方法的必要性。本文提出GhostSpec——一种轻量级但有效的LLM谱系验证方法，无需访问训练数据或改变模型行为。该方法通过对内部注意力权重矩阵的不变量乘积应用奇异值分解（SVD），构建紧凑而鲁棒的指纹，有效捕捉模型的结构特征。与水印或基于输出的方法不同，GhostSpec完全无需数据、非侵入式且计算高效，对连续微调、剪枝、块扩展乃至对抗性变换均表现出强鲁棒性。大量实验表明，GhostSpec能以最小开销可靠追溯变换后模型的谱系。通过为模型验证和复用追踪提供实用解决方案，本方法有助于保护知识产权，并促进大规模语言模型形成透明可信的生态系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement and widespread adoption of Large Language Models (LLMs) have raised concerns about intellectual property protection, as some developers falsely claim original training of models derived from existing open-source ones. To address this, the authors propose GhostSpec, a lightweight method that constructs compact fingerprints by applying singular value decomposition to invariant products of internal attention weight matrices, capturing the structural identity of models without requiring training data or altering model behavior. Experimental results demonstrate that GhostSpec reliably traces model lineage under various transformations, including sequential fine-tuning, pruning, block expansion, and adversarial attacks, offering a robust solution for verifying model provenance and promoting transparency in the LLM ecosystem.</div>
<div class="mono" style="margin-top:8px">随着大语言模型的快速普及，部分开发者基于开源模型微调后却声称独立训练，引发了知识产权保护的迫切需求。为此，研究者提出GhostSpec方法，通过对内部注意力权重矩阵的不变乘积进行奇异值分解，构建轻量而鲁棒的模型指纹，无需训练数据或改变模型行为即可捕捉结构特征。实验表明，该方法在连续微调、剪枝、块扩展及对抗性变换等多种场景下均能可靠追溯模型谱系，为验证模型来源和保护知识产权提供了实用解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</div>
<div class="meta-line">Authors: Yuhang Peng, Yizhou Pan, Xinning He, Jihaoyu Yang, Xinyu Yin, Han Wang, Xiaoji Zheng, Chao Gao, Jiangtao Gong</div>
<div class="meta-line">Venue: AAAI 2026 Oral</div>
<div class="meta-line">First: 2025-11-17T15:58:46+00:00 · Latest: 2025-11-17T15:58:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13524v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自由问答世界：以人为中心的具身人工智能交互式闭环模拟器</div>
<div class="mono" style="margin-top:8px">随着具身智能成为人工智能研究的核心前沿，仿真平台需从底层物理交互向复杂的人本社会行为演进。我们提出自由问答世界——一个融合大语言模型进行高层行为规划与语义 grounded 交互的仿真框架，其设计基于意图理论与社会认知理论。该框架支持可扩展的逼真人机交互仿真，并包含面向多样化具身任务的模块化数据生成管线。为验证框架性能，我们将经典视觉语言导航任务扩展为交互增强的定向问询场景，使智能体能够主动寻求并解析导航指引。我们公开推出自由问答世界大规模基准数据集，包含重构环境、六类任务型态、16个核心对象类别、63,429帧标注样本及逾17小时交互数据，以支持具身AI系统的训练与评估。通过对VLN模型及人类参与者在开环与闭环设置下的基准测试，实验表明经自由问答世界微调的模型在语义理解与交互能力上均优于原模型。这些发现印证了基于社会情境的仿真框架在推动具身AI系统实现高级规划与自然人机交互方面的有效性。特别指出，本研究揭示了交互本身可作为独立的信息模态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As embodied intelligence advances, simulation platforms need to capture complex human-centered social behaviors beyond basic physical interactions. This research introduces FreeAskWorld, an interactive simulation framework that integrates large language models for high-level behavior planning and semantically grounded interactions based on intention and social cognition theories, featuring a modular data generation pipeline for diverse embodied tasks. Experimental validation extends the Vision-and-Language Navigation task into a Direction Inquiry setting, where agents actively seek navigational guidance; results show that models fine-tuned on the FreeAskWorld dataset outperform original models, achieving improved semantic understanding and interaction competency, highlighting interaction as an additional information modality for advancing embodied AI.</div>
<div class="mono" style="margin-top:8px">为满足具身人工智能中对复杂人本社会行为模拟的需求，本研究提出了FreeAskWorld交互式框架，集成大语言模型实现高层行为规划和语义基础交互。该方法将视觉语言导航任务扩展为方向询问场景，使智能体能够主动寻求导航指导，并采用模块化数据生成流程。实验结果表明，基于FreeAskWorld微调的模型优于原始模型，实现了更强的语义理解和交互能力，证实了交互本身作为额外信息模态的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Language-Guided Invariance Probing of Vision-Language Models</div>
<div class="meta-line">Authors: Jae Joong Lee</div>
<div class="meta-line">First: 2025-11-17T15:35:49+00:00 · Latest: 2025-11-17T15:35:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13494v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13494v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言引导的视觉语言模型不变性探测</div>
<div class="mono" style="margin-top:8px">近期视觉语言模型（如CLIP、OpenCLIP、EVA02-CLIP和SigLIP）展现出强大的零样本性能，但其对受控语言扰动的响应可靠性尚不明确。我们提出语言引导不变性探测基准，通过四万张MS COCO图像及其人工标注，自动生成保持语义的复述与改变物体类别/颜色/数量的规则化语义翻转，从图像-文本匹配角度量化模型对语义保留的稳定性与语义变化的敏感性。实验发现：EVA02-CLIP与大型OpenCLIP变体在不变性-敏感性边界表现优异，既保持低复述方差，又对原始标注的评分持续高于翻转版本；而SigLIP系列则呈现较高的不变性误差，且频繁偏好翻转后的描述。这些缺陷在传统检索指标中难以察觉，表明该基准能为视觉语言模型的语言鲁棒性提供超越常规准确率的模型无关诊断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need to evaluate the linguistic robustness of vision-language models (VLMs) beyond standard zero-shot metrics by examining their invariance to paraphrases and sensitivity to semantic changes. The authors introduce Language-Guided Invariance Probing (LGIP), a benchmark that uses 40k MS COCO images with human captions to automatically generate meaning-preserving paraphrases and rule-based semantic flips altering object category, color, or count, then computes invariance error, semantic sensitivity gap, and positive-rate statistics. Experiments on nine VLMs reveal that EVA02-CLIP and large OpenCLIP variants achieve a favorable balance with low paraphrase variance and higher scores for original captions over flipped ones, whereas SigLIP and SigLIP2 exhibit high invariance error and often prefer flipped captions, particularly for object and color edits, highlighting LGIP&#x27;s diagnostic value beyond conventional retrieval metrics.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在零样本任务中表现优异但语言响应可靠性不明的问题，提出了语言引导不变性探测基准，通过自动生成MS COCO描述文本的语义保留改写和语义翻转（如修改物体类别、颜色或数量），系统评估模型对语义不变性的保持能力和对语义变化的敏感度。实验发现，EVA02-CLIP与大型OpenCLIP变体在不变性与敏感性间取得了较优平衡，而SigLIP系列模型表现出较高的不变性误差且频繁偏好翻转文本，尤其在物体和颜色修改中，这些缺陷在传统检索指标中难以察觉，凸显了该基准对模型语言鲁棒性的诊断价值。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling</div>
<div class="meta-line">Authors: Adam Hazimeh, Ke Wang, Mark Collier, Gilles Baechler, Efi Kokiopoulou, Pascal Frossard</div>
<div class="meta-line">First: 2025-11-17T15:16:13+00:00 · Latest: 2025-11-17T15:16:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13478v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13478v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义文档逆向渲染：基于视觉语言建模的SVG重构</div>
<div class="mono" style="margin-top:8px">幻灯片演示文稿与海报等多媒体文档本应具备交互性和易修改性，但常以静态栅格格式传播，限制了编辑与定制功能。要恢复其可编辑性，需将这些栅格图像转换回结构化矢量格式。然而，现有基于曲线和多边形等低层图元的几何栅格-矢量化方法存在局限：在处理幻灯片等复杂文档时，它们难以保持高层结构，导致生成扁平的形状集合，使图像与文本元素间的语义区分丧失。为此，我们通过引入SliDer框架解决语义文档逆向渲染问题——该创新框架利用视觉语言模型将幻灯片图像逆向渲染为紧凑可编辑的可缩放矢量图形。SliDer从栅格输入中检测并提取单个图像与文本元素的属性，将其组织为连贯的SVG格式。关键的是，该模型在推理过程中通过类人设计流程迭代优化预测，生成的SVG代码在渲染时能更精准地重构原始栅格。此外，我们推出了Slide2SVG数据集，包含从真实科学演示中采集的幻灯片文档栅格-SVG配对数据，以推动该领域研究。实验表明，SliDer实现了0.069的重构LPIPS指标，在82.9%的案例中被人类评估者认为优于最强的零样本视觉语言模型基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of restoring editability to static raster documents like slides and posters by converting them into structured vector formats, as existing geometric raster-vectorization methods fail to preserve high-level semantic structure. The authors propose SliDer, a framework that leverages Vision-Language Models to detect and extract attributes from text and image elements in raster inputs, iteratively refining predictions to generate coherent and editable SVG representations. Experimental results show that SliDer achieves a reconstruction LPIPS of 0.069 and is preferred by human evaluators in 82.9% of cases over the strongest zero-shot baseline, demonstrating its effectiveness in semantic document derendering.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决静态栅格文档（如幻灯片）恢复可编辑性的问题，因为现有基于几何的栅格矢量化方法无法保留高层次语义结构。作者提出SliDer框架，利用视觉语言模型检测并提取栅格输入中的文本和图像元素属性，通过迭代优化预测生成连贯且可编辑的SVG表示。实验结果表明，SliDer实现了0.069的重建LPIPS指标，并在82.9%的案例中被人类评估者认为优于最强的零样本基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Trust in Vision-Language Models: Insights from a Participatory User Workshop</div>
<div class="meta-line">Authors: Agnese Chiatti, Lara Piccolo, Sara Bernardini, Matteo Matteucci, Viola Schiaffonati</div>
<div class="meta-line">Venue: Proceedings of the The European Workshop on Trustworthy AI (Trust-AI) at ECAI 2025</div>
<div class="meta-line">First: 2025-11-17T15:04:59+00:00 · Latest: 2025-11-17T15:04:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13458v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants&#x27; engagement to fit the case of user-VLM interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对视觉语言模型的信任：来自参与式用户研讨会的见解</div>
<div class="mono" style="margin-top:8px">随着基于大规模图像文本和视频文本数据集预训练的视觉语言模型日益普及，为用户提供辨别何时信任这些系统的工具至关重要。然而，探究用户对VLM的信任如何建立与演变仍是一个待解难题。当研究者为规避直接开展参与式设计研究产生的成本与影响，日益依赖将AI模型作为实验验证的评判标准时，该问题更显突出。本文遵循以用户为中心的研究方法，展示了与潜在VLM用户开展研讨会的初步成果。该试点研讨会的洞见为未来研究指明方向，旨在将信任度量指标与参与者参与策略进行情境化适配，以契合用户与VLM交互的具体场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As Vision-Language Models (VLMs) are increasingly deployed, it is essential to understand how users develop trust in these systems, especially given the reliance on AI models for validation instead of direct user studies. This study adopts a user-centered approach by conducting a participatory workshop with prospective VLM users to explore trust dynamics. Preliminary findings from the workshop provide insights for future research on contextualizing trust metrics and engagement strategies tailored to user-VLM interactions.</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型的广泛应用，理解用户信任如何形成对于帮助用户判断何时依赖这些系统至关重要。本研究采用以用户为中心的方法，通过组织潜在用户的参与式工作坊来探讨信任动态。工作坊的初步结果为未来研究提供了见解，旨在情境化信任指标并设计适合用户与模型交互的参与策略。</div>
</details>
</div>
<div class="card">
<div class="title">Vision Transformers with Self-Distilled Registers</div>
<div class="meta-line">Authors: Yinjie Chen, Zipeng Yan, Chong Zhou, Bo Dai, Andrew F. Luo</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-27T17:59:41+00:00 · Latest: 2025-11-17T15:02:58+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight. Website: https://github.com/0raiser0/PH-Reg</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.21501v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.21501v2">PDF</a> · <a href="https://github.com/0raiser0/PH-Reg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is the addition of register tokens to ViTs, which implicitly &quot;absorb&quot; the artifact term during training.Given the availability of existing large-scale pre-trained ViTs, in this paper we seek add register tokens to existing models without needing to re-train from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher&#x27;s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有自蒸馏寄存器的视觉Transformer</div>
<div class="mono" style="margin-top:8px">视觉Transformer已成为视觉处理任务的主流架构，展现出随训练数据和模型规模扩大而持续优化的卓越扩展性。然而近期研究发现在ViT中会出现与局部语义不一致的伪影标记，这些异常标记会损害需要细粒度定位或结构连贯性的任务性能。通过引入寄存器标记可有效缓解该问题，使其在训练过程中隐式“吸收”伪影项。鉴于现有大规模预训练ViT的普及，本文致力于在不重新训练的前提下为已有模型添加寄存器——考虑到模型规模，完整重训练并不现实。具体而言，我们提出事后寄存器（PH-Reg），这是一种高效的自蒸馏方法，无需额外标注数据和完整重训练即可将寄存器集成到现有ViT中。该方法基于同一预训练ViT初始化教师网络与学生网络：教师网络保持冻结未改动，学生网络则添加随机初始化的寄存器标记。通过对教师网络输入实施测试时增强，我们生成无伪影的降噪密集嵌入，进而仅优化学生网络中少量解锁参数。实验表明该方法能有效减少伪影标记数量，在零样本和线性探测场景下显著提升学生ViT的分割与深度预测性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) exhibit performance degradation due to artifact tokens that disrupt local semantics, particularly in tasks requiring fine-grained localization. To address this without costly full retraining of large pre-trained models, the authors propose Post Hoc Registers (PH-Reg), a self-distillation method that adds register tokens to an existing ViT. The approach initializes identical teacher and student networks from a pre-trained ViT, freezes the teacher, augments the student with registers, and uses test-time augmentation to generate denoised embeddings for optimizing a small subset of student weights. Experimental results demonstrate that PH-Reg effectively reduces artifact tokens and improves zero-shot segmentation and depth prediction performance.</div>
<div class="mono" style="margin-top:8px">视觉Transformer（ViT）存在伪影标记问题，损害细粒度定位和结构一致性，因此需要在无需完整重训练的情况下为预训练模型集成寄存器标记。本文提出事后寄存器（PH-Reg）方法，采用自蒸馏策略，从同一预训练ViT初始化师生网络，为学生网络添加随机初始化的寄存器，并通过对教师网络进行测试时增强生成去噪嵌入，以优化学生网络的小部分权重。实验结果表明，该方法能有效减少伪影标记，提升零样本分割和深度预测在线性探测下的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline</div>
<div class="meta-line">Authors: Rui Zuo, Qinyue Tong, Zhe-Ming Lu, Ziqian Lu</div>
<div class="meta-line">First: 2025-11-17T14:49:57+00:00 · Latest: 2025-11-17T14:49:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13442v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解锁基础MLLMs的伪造检测潜力：一种新型免训练流程</div>
<div class="mono" style="margin-top:8px">随着人工智能生成内容（AIGC）技术的飞速发展，包括多模态大语言模型（MLLMs）和扩散模型在内的技术使得图像生成与篡改变得异常轻松。现有图像伪造检测与定位（IFDL）方法常难以跨数据集泛化，且可解释性有限。当前MLLMs在多种视觉语言任务中展现出强大泛化潜力，部分研究通过大规模训练将这种能力引入IFDL领域。然而这类方法消耗大量算力资源，却未能揭示基础MLLMs解决该问题的内在泛化潜力。受此启发，我们提出Foresee——专为图像伪造分析设计的免训练MLLM流程。该方法无需额外训练即可实现轻量级推理，同时在篡改定位精度和文本解释丰富性方面超越现有基于MLLM的方法。Foresee采用类型先验驱动策略，并利用灵活特征检测器（FFD）模块专门处理复制-移动篡改，从而有效释放基础MLLMs在取证领域的潜力。大量实验表明，我们的方法在实现卓越定位精度的同时能提供更全面的文本解释。此外，Foresee展现出更强的泛化能力，在复制-移动、拼接、移除、局部增强、深度伪造及基于AIGC的编辑等多种篡改类型上均优于现有IFDL方法。代码将在最终版本中发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing image forgery detection and localization methods in generalization and interpretability, this study introduces Foresee, a training-free pipeline that leverages vanilla multimodal large language models (MLLMs) for forensic analysis. The method employs a type-prior-driven strategy and a Flexible Feature Detector module to specifically address copy-move manipulations without requiring additional training. Experimental results show that Foresee achieves superior localization accuracy and provides richer textual explanations, while demonstrating strong generalization across various tampering types such as copy-move, splicing, removal, deepfake, and AIGC-based editing.</div>
<div class="mono" style="margin-top:8px">针对现有图像伪造检测与定位方法在泛化性和可解释性方面的不足，本研究提出了Foresee，一种无需训练、利用原始多模态大语言模型进行取证分析的流程。该方法采用类型先验驱动策略和灵活特征检测器模块处理复制-移动篡改，无需额外训练即可实现轻量级推理。实验结果表明，Foresee在多种篡改类型（包括复制-移动、拼接、移除和基于AIGC的编辑）上均取得了更优的定位精度和更丰富的文本解释，同时展现出比现有方法更强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task</div>
<div class="meta-line">Authors: Xingming Long, Jie Zhang, Shiguang Shan, Xilin Chen</div>
<div class="meta-line">First: 2025-11-17T14:32:06+00:00 · Latest: 2025-11-17T14:32:06+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13420v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13420v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs&#x27; hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model&#x27;s interpretation and the object&#x27;s presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOPE：重新审视视觉语言模型在自主想象任务中的幻觉现象</div>
<div class="mono" style="margin-top:8px">当前针对大型视觉语言模型（LVLM）幻觉的研究主要集中于禁止输出图像外内容的事实描述任务，而对自主想象任务（如故事创作）中的幻觉关注不足。这类任务要求模型生成超越给定图像的新内容，因此不宜简单将想象内容视为幻觉。为此，我们提出自主想象对象存在性评估（VOPE）——通过存在性评估衡量LVLM在自主想象任务中幻觉的新方法。具体而言，VOPE提出基于复核的问题，评估LVLM如何解释其响应中想象对象的存在性。模型解释与图像中对象实际存在性的一致性，用于判定模型生成响应时是否产生幻觉。我们将VOPE应用于多个主流LVLM及幻觉缓解方法，发现：（1）多数LVLM在自主想象时存在严重幻觉，其对想象对象的存在性评估表现显著较差；（2）现有幻觉缓解方法在自主想象任务中效果有限，这成为未来研究的重要方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current research on hallucinations in Large Vision-Language Models primarily targets factual description tasks, neglecting voluntary imagination scenarios like story writing where models are expected to generate novel content beyond the given image. To address this gap, the authors propose Voluntary-imagined Object Presence Evaluation (VOPE), a method that assesses hallucinations by posing recheck-based questions to evaluate how models interpret the presence of imagined objects in their responses, using consistency with the image to determine hallucination. Experimental results on mainstream LVLMs and mitigation methods reveal that most models hallucinate heavily during voluntary imagination and perform poorly in presence evaluation for imagined objects, while existing mitigation techniques show limited effectiveness, highlighting the need for future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型在自愿想象任务（如故事创作）中的幻觉问题展开探讨，这类任务期望模型生成图像之外的新内容而非禁止。作者提出VOPE方法，通过提出复查式问题评估模型如何解释其响应中想象对象的存在，并将此解释与图像实际内容对比以检测不一致性。对主流模型和缓解方法的实验表明，多数模型在此类任务中产生大量幻觉且在存在性评估上表现较差，现有缓解方法效果有限，这指明了未来研究的重要方向。</div>
</details>
</div>
<div class="card">
<div class="title">Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)</div>
<div class="meta-line">Authors: Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising</div>
<div class="meta-line">First: 2025-11-17T14:12:22+00:00 · Latest: 2025-11-17T14:12:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13397v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13397v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>描述符：距离标注交通感知问答（DTPQA）</div>
<div class="mono" style="margin-top:8px">视觉语言模型在多任务中的显著进展引发了其在自动驾驶领域应用的关注。然而，要让这些模型在安全关键领域获得信任，首先需具备稳健的感知能力——即能理解常具高度复杂性、多事件并发的交通场景。鉴于交通场景中的关键物体与智能体常处于远距离，我们要求系统不仅具备近距离（20米内）的强感知能力，还需拥有远距离（30米以上）感知性能。因此，有必要将这些模型的感知能力与推理、世界知识等其他技能分开评估。距离标注交通感知问答（DTPQA）是专为此设计的视觉问答基准：通过驾驶决策相关的简单但关键问题，评估视觉语言模型在交通场景中的感知系统。该基准包含基于模拟器构建的合成数据集（DTP-Synthetic）和基于真实交通场景图像构建的现实数据集（DTP-Real）。DTPQA还包含距离标注，即目标物体与摄像头的距离。具体而言，每个DTPQA样本至少包含：（a）图像（b）问题（c）真实答案（d）目标物体距离，以此分析视觉语言模型性能随距离增加而衰减的规律。本文同时提供了数据集及其生成所需的Python脚本，可用于生成同类附加数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to rigorously evaluate vision-language models&#x27; perception capabilities in automated driving, where safety-critical objects often appear at long distances. The authors introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), a visual question answering benchmark comprising synthetic data generated via simulation and real-world traffic images, with each sample including distance annotations for objects. Experimental results demonstrate that DTPQA enables systematic analysis of how model performance degrades as object distance increases, providing crucial insights for developing robust perception systems.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶中安全关键物体常出现在远距离的场景，提出需要严格评估视觉语言模型的感知能力。作者开发了距离标注交通感知问答基准（DTPQA），包含模拟器生成的合成数据和真实交通图像，并通过距离标注分离感知与推理能力。实验结果表明，DTPQA能有效衡量模型性能随物体距离增加而下降的规律，为评估交通场景理解提供了专用工具。</div>
</details>
</div>
<div class="card">
<div class="title">Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce&#x27;s Manuscripts with Vision-Language Models</div>
<div class="meta-line">Authors: Carlo Teo Pedretti, Davide Picca, Dario Rodighiero</div>
<div class="meta-line">First: 2025-11-17T13:52:23+00:00 · Latest: 2025-11-17T13:52:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13378v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce&#x27;s semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维动态图像：运用视觉语言模型从查尔斯·S·皮尔士手稿中提取视觉知识</div>
<div class="mono" style="margin-top:8px">图表作为多学科领域至关重要却未获充分探索的工具，展现了视觉表征与学术推理间的紧密联系。然而其图标形态为视觉研究、跨媒介分析及基于文本的数字化工作流程带来障碍。查尔斯·S·皮尔士始终主张图表对推理与阐释具有关键作用，其手稿常融合文本内容与复杂视觉元素，为研究异质材料构成的文献提供了挑战性案例。本初步研究探讨视觉语言模型能否有效协助我们在语境中识别并解读此类混合页面：首先提出包含（i）手稿页面布局分割（ii）各片段与IIIF标准标注重新关联（iii）将含图表片段提交VLM处理的工作流程；其次借鉴皮尔士符号学框架设计提示模板，提取图表核心知识并生成简明标注；最终将这些标注整合至知识图谱，实现复合资源中图表内容的结构化表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of analyzing hybrid manuscript pages that combine text and diagrams, which are crucial for understanding scholarly reasoning but difficult to process with conventional text-based methods. The authors developed a workflow using Vision-Language Models (VLMs) to segment manuscript layouts, link segments to annotations, and analyze diagram-containing fragments through semiotically-informed prompts. Experimental results demonstrate that this approach successfully extracts key diagrammatic knowledge and generates concise captions, which are then integrated into structured knowledge graphs for representing composite document content.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决混合手稿页面中文本与图表结合的分析难题，这些图表对理解学术推理至关重要，但传统文本方法难以处理。作者提出一种利用视觉语言模型的工作流程，通过分割手稿布局、关联注释片段，并应用符号学启发的提示来提取图表知识并生成描述。实验结果表明，该方法能有效识别和解释视觉元素，将图表内容整合到结构化知识图谱中，从而提升对复合文档的分析能力。</div>
</details>
</div>
<div class="card">
<div class="title">LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit</div>
<div class="meta-line">Authors: Chengtao Lv, Bilang Zhang, Yang Yong, Ruihao Gong, Yushi Huang, Shiqiao Gu, Jiajun Wu, Yumeng Shi, Jinyang Guo, Wenya Wang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-13T17:54:49+00:00 · Latest: 2025-11-17T13:22:24+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09981v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09981v2">PDF</a> · <a href="https://github.com/ModelTC/LightCompress">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) exhibit impressive multi-modal capabilities but suffer from prohibitive computational and memory demands, due to their long visual token sequences and massive parameter sizes. To address these issues, recent works have proposed training-free compression methods. However, existing efforts often suffer from three major limitations: (1) Current approaches do not decompose techniques into comparable modules, hindering fair evaluation across spatial and temporal redundancy. (2) Evaluation confined to simple single-turn tasks, failing to reflect performance in realistic scenarios. (3) Isolated use of individual compression techniques, without exploring their joint potential. To overcome these gaps, we introduce LLMC+, a comprehensive VLM compression benchmark with a versatile, plug-and-play toolkit. LLMC+ supports over 20 algorithms across five representative VLM families and enables systematic study of token-level and model-level compression. Our benchmark reveals that: (1) Spatial and temporal redundancies demand distinct technical strategies. (2) Token reduction methods degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3) Combining token and model compression achieves extreme compression with minimal performance loss. We believe LLMC+ will facilitate fair evaluation and inspire future research in efficient VLM. Our code is available at https://github.com/ModelTC/LightCompress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMC+：利用即插即用工具包对视觉语言模型压缩进行基准测试</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型展现出卓越的多模态能力，但由于其冗长的视觉标记序列和海量参数规模，存在计算和内存需求过高的问题。为应对这些挑战，近期研究提出了免训练的压缩方法。然而现有工作存在三大局限：(1) 当前方法未将技术解构为可比模块，阻碍了空间与时间冗余的公平评估；(2) 评估局限于简单单轮任务，无法反映真实场景性能；(3) 孤立使用单一压缩技术，未探索联合潜力。为此，我们推出LLMC+——配备多功能即插即用工具包的综合性VLM压缩基准，支持五大代表性VLM家族的20余种算法，实现标记级与模型级压缩的系统研究。我们的基准揭示：(1) 空间与时间冗余需采用差异化技术策略；(2) 标记削减方法在多轮对话和细节敏感任务中性能显著下降；(3) 结合标记与模型压缩可实现极致压缩且性能损失最小。我们相信LLMC+将促进公平评估并推动高效VLM的未来研究。代码已开源：https://github.com/ModelTC/LightCompress。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models face high computational and memory costs from long visual tokens and large parameters, motivating the need for efficient compression. This work introduces LLMC+, a plug-and-play toolkit that benchmarks over 20 training-free compression algorithms across five VLM families, systematically evaluating token-level and model-level methods. Key findings show that spatial and temporal redundancies require different strategies, token reduction underperforms in multi-turn and detail-sensitive tasks, and combining token with model compression achieves extreme compression with minimal performance loss.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型因长视觉标记和庞大参数导致高昂计算成本，亟需无需训练的压缩方法。本研究提出LLMC+，一个即插即用工具包，在五大VLM系列上评估20多种压缩算法，系统研究标记级和模型级压缩。主要发现包括：空间与时间冗余需不同策略，标记削减在多轮对话和细节敏感任务中表现不佳，而结合标记与模型压缩可在极低性能损失下实现高效压缩。</div>
</details>
</div>
<div class="card">
<div class="title">Tab-PET: Graph-Based Positional Encodings for Tabular Transformers</div>
<div class="meta-line">Authors: Yunze Leng, Rohan Ghosh, Mehul Motani</div>
<div class="meta-line">First: 2025-11-17T13:08:34+00:00 · Latest: 2025-11-17T13:08:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13338v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13338v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tab-PET：基于图的位置编码在表格Transformer中的应用</div>
<div class="mono" style="margin-top:8px">表格数据的监督学习面临独特挑战，包括数据量小、缺乏结构线索以及跨越分类与连续域的异构特征。与可利用数据归纳偏置的视觉和语言任务不同，表格数据缺乏内在位置结构，这限制了自注意力机制的有效性。尽管近期基于Transformer的模型（如TabTransformer、SAINT和FT-Transformer，合称3T）在表格数据上展现出潜力，但它们通常未利用位置编码等结构线索，因为先验结构信息往往不可得。本工作通过理论与实验证明，结构线索（特别是位置编码）能有效提升表格Transformer的泛化性能。我们发现位置编码可通过降低特征有效秩（内在维度的一种形式）来简化任务维度，从而改善泛化能力。为此，我们提出Tab-PET（表格Transformer位置编码框架），这是一个基于图的框架，用于估计并将位置编码融入嵌入表示。受从图拓扑推导位置编码的方法启发，我们探索了两种图估计范式：基于关联性与基于因果性。我们在50个分类和回归数据集上的实验表明，基于图的位置编码能显著提升3T模型性能。值得注意的是，与因果驱动方法相比，基于关联性的图始终能带来更稳定显著的性能提升。本研究揭示了位置编码在表格Transformer中的意外作用，展现了其提升泛化能力的新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Supervised learning with tabular data faces challenges such as limited data size, lack of structural cues, and heterogeneous features, which hinder the effectiveness of self-attention mechanisms in transformers. To address this, Tab-PET introduces a graph-based framework that estimates positional encodings from association-based or causality-based graphs and integrates them into transformer embeddings, reducing the effective rank of features to simplify the task. Experimental results on 50 classification and regression datasets show that graph-derived positional encodings significantly enhance performance, with association-based graphs providing more stable and pronounced improvements compared to causality-driven approaches.</div>
<div class="mono" style="margin-top:8px">表格数据的监督学习面临数据量小、缺乏结构线索以及特征异构等挑战，限制了自注意力机制在Transformer模型中的有效性。为此，Tab-PET提出了一种基于图的框架，通过关联图或因果图估计位置编码并融入嵌入表示，以降低特征的有效秩从而简化任务。在50个分类和回归数据集上的实验表明，基于图的位置编码显著提升了模型性能，其中关联图方法比因果图方法带来了更稳定和显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Certified Coil Geometry Learning for Short-Range Magnetic Actuation and Spacecraft Docking Application</div>
<div class="meta-line">Authors: Yuta Takahashi, Hayate Tajima, Shin-ichiro Sakai</div>
<div class="meta-line">First: 2025-07-04T20:54:30+00:00 · Latest: 2025-11-17T12:36:41+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Robotics and Automation Letters</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.03806v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.03806v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a learning-based framework for approximating an exact magnetic-field interaction model, supported by both numerical and experimental validation. High-fidelity magnetic-field interaction modeling is essential for achieving exceptional accuracy and responsiveness across a wide range of fields, including transportation, energy systems, medicine, biomedical robotics, and aerospace robotics. In aerospace engineering, magnetic actuation has been investigated as a fuel-free solution for multi-satellite attitude and formation control. Although the exact magnetic field can be computed from the Biot-Savart law, the associated computational cost is prohibitive, and prior studies have therefore relied on dipole approximations to improve efficiency. However, these approximations lose accuracy during proximity operations, leading to unstable behavior and even collisions. To address this limitation, we develop a learning-based approximation framework that faithfully reproduces the exact field while dramatically reducing computational cost. The proposed method additionally provides a certified error bound, derived from the number of training samples, ensuring reliable prediction accuracy. The learned model can also accommodate interactions between coils of different sizes through appropriate geometric transformations, without retraining. To verify the effectiveness of the proposed framework under challenging conditions, a spacecraft docking scenario is examined through both numerical simulations and experimental validation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向短程磁驱动与航天器对接应用的认证线圈几何学习</div>
<div class="mono" style="margin-top:8px">本文提出一种基于学习的框架，用于逼近精确磁场相互作用模型，并通过数值模拟与实验验证共同支持。高保真磁场相互作用建模对于在交通运输、能源系统、医学、生物医学机器人及航空航天机器人等广泛领域实现卓越精度与响应能力至关重要。在航天工程中，磁驱动作为无燃料解决方案已被研究用于多卫星姿态与编队控制。虽然可通过毕奥-萨伐尔定律计算精确磁场，但其计算成本过高，故先前研究多采用偶极子近似提升效率。然而这些近似在近距离操作时精度下降，导致不稳定行为甚至碰撞。为解决该局限，我们开发的学习型近似框架在显著降低计算成本的同时能忠实复现精确磁场。该方法还通过训练样本数推导出认证误差界，确保可靠的预测精度。所学模型还能通过适当几何变换适应不同尺寸线圈间的相互作用，无需重新训练。为验证所提框架在挑战性条件下的有效性，通过数值仿真与实验验证对航天器对接场景进行了检验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the computational inefficiency of exact magnetic field modeling via Biot-Savart law and the inaccuracy of dipole approximations during proximity operations in aerospace applications. The authors develop a learning-based framework that approximates the exact magnetic field with certified error bounds, significantly reducing computational costs while maintaining accuracy, and generalizing to coils of different sizes without retraining. Experimental validation in spacecraft docking scenarios confirms the method&#x27;s effectiveness under challenging conditions, ensuring stable and collision-free operations.</div>
<div class="mono" style="margin-top:8px">本研究针对航空航天应用中毕奥-萨伐尔定律精确磁场建模计算效率低、以及近距离操作中偶极子近似精度不足的问题，开发了一种基于学习的磁场近似框架。该方法通过认证误差边界保证预测可靠性，大幅降低计算成本，并能通过几何变换适应不同尺寸线圈的相互作用而无需重新训练。在航天器对接场景中的实验验证表明，该框架在挑战性条件下能有效确保稳定无碰撞操作。</div>
</details>
</div>
<div class="card">
<div class="title">TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing</div>
<div class="meta-line">Authors: Jongha Kim, Minseong Bae, Sanghyeok Lee, Jinsung Yoon, Hyunwoo J. Kim</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-17T12:00:23+00:00 · Latest: 2025-11-17T12:00:23+00:00</div>
<div class="meta-line">Comments: AAAI 2026 (Main Technical Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13283v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13283v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer&#x27;s capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TabFlash：通过渐进式问题条件化与令牌聚焦实现高效表格理解</div>
<div class="mono" style="margin-top:8px">表格图像因需针对特定问题聚焦且存在冗余背景区域，给有效且高效的理解带来独特挑战。现有多模态大语言模型方法常忽视这些特性，导致视觉表征信息量不足且冗余。为解决这些问题，我们致力于生成兼具信息性与紧凑性的视觉特征以改进表格理解。首先提出渐进式问题条件化，根据各层处理附加信息的能力，以逐层递增的频率将问题注入视觉Transformer层，生成问题感知的视觉特征。为降低冗余，引入剪枝策略剔除背景令牌以提升效率。为缓解剪枝造成的信息损失，进一步提出令牌聚焦训练策略，促使模型将关键信息浓缩于保留令牌中。通过结合这些方法，我们推出TabFlash——一个高效精准的表格理解多模态大语言模型。TabFlash实现了最先进性能，超越开源与专有模型，同时相较次优模型减少27%浮点运算量与30%内存占用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the inefficiency of existing Multimodal Large Language Models in table image understanding, where redundant background regions and lack of question-specific focus lead to uninformative representations. The proposed TabFlash method introduces progressive question conditioning to inject question information into Vision Transformer layers adaptively, alongside a pruning strategy to remove background tokens and token focusing to preserve essential information in retained tokens. Experiments demonstrate state-of-the-art performance, surpassing both open-source and proprietary models while reducing computational costs by 27% in FLOPs and 30% in memory usage compared to the second-best method.</div>
<div class="mono" style="margin-top:8px">表格图像因冗余背景区域和问题特定聚焦需求给多模态理解带来挑战，现有MLLM常忽略这些特性，导致表示效率低下。为解决此问题，作者提出渐进式问题条件化，以递增频率将问题注入Vision Transformer层生成问题感知特征，结合剪枝策略去除背景令牌提升效率，并通过训练中的令牌聚焦将关键信息集中于保留令牌。实验结果表明，TabFlash实现了最先进性能，超越开源和专有MLLM，相比次优模型减少27%计算量和30%内存使用。</div>
</details>
</div>
<div class="card">
<div class="title">Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency</div>
<div class="meta-line">Authors: Bo Yang, Hengwei Zhang, Jindong Wang, Yuchen Ren, Chenhao Lin, Chao Shen, Zhengyu Zhao</div>
<div class="meta-line">First: 2025-05-19T02:56:41+00:00 · Latest: 2025-11-17T11:44:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12644v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In surrogate ensemble attacks, using more surrogate models yields higher transferability but lower resource efficiency. This practical trade-off between transferability and efficiency has largely limited existing attacks despite many pre-trained models are easily accessible online. In this paper, we argue that such a trade-off is caused by an unnecessary common assumption, i.e., all models should be \textit{identical} across iterations. By lifting this assumption, we can use as many surrogates as we want to unleash transferability without sacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA), which dynamically selects diverse models (from easily accessible pre-trained models) across iterations based on our new interpretation of decoupling within-iteration and cross-iteration model diversity. In this way, the number of within-iteration models is fixed for maintaining efficiency, while only cross-iteration model diversity is increased for higher transferability. Experiments on ImageNet demonstrate the superiority of SEA in various scenarios. For example, when dynamically selecting 4 from 20 accessible models, SEA yields 8.5% higher transferability than existing attacks under the same efficiency. The superiority of SEA also generalizes to real-world systems, such as commercial vision APIs and large vision-language models. Overall, SEA opens up the possibility of adaptively balancing transferability and efficiency according to specific resource requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随心所欲使用代理模型：选择性集成攻击在不牺牲资源效率的前提下释放迁移性</div>
<div class="mono" style="margin-top:8px">在代理集成攻击中，使用更多代理模型虽能提升迁移性但会降低资源效率。这种迁移性与效率间的现实权衡极大限制了现有攻击方法的发展，尽管在线可获取的预训练模型数量众多。本文指出该权衡源于一个不必要的普遍假设——所有模型在迭代间应保持完全一致。通过突破该假设，我们能在不牺牲效率的前提下任意使用大量代理模型来释放迁移性。具体而言，我们提出选择性集成攻击（SEA），基于对迭代内与跨迭代模型多样性的新解耦阐释，在迭代过程中动态选择多样化模型（从易获取的预训练模型中）。该方法通过固定迭代内模型数量维持效率，同时仅增加跨迭代模型多样性以提升迁移性。ImageNet上的实验证明了SEA在多场景下的优越性：当从20个可用模型中动态选择4个时，SEA在同等效率下比现有攻击提升8.5%的迁移性。SEA的优越性还泛化至现实系统，如商业视觉API和大型视觉语言模型。总体而言，SEA为根据具体资源需求自适应平衡迁移性与效率开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the trade-off between transferability and resource efficiency in surrogate ensemble attacks, where using more models improves transferability but reduces efficiency. The proposed Selective Ensemble Attack (SEA) dynamically selects diverse pre-trained models across iterations by decoupling within-iteration and cross-iteration model diversity, maintaining a fixed number of models per iteration for efficiency while increasing cross-iteration diversity for enhanced transferability. Experiments on ImageNet show that SEA achieves 8.5% higher transferability than existing methods under identical efficiency when selecting 4 from 20 models, with effectiveness extending to commercial vision APIs and large vision-language models.</div>
<div class="mono" style="margin-top:8px">本研究针对替代模型集成攻击中迁移性与资源效率之间的权衡问题展开，传统方法使用更多模型可提升迁移性但降低效率。提出的选择性集成攻击（SEA）通过解耦迭代内和迭代间模型多样性，在迭代中动态选择多样化的预训练模型，保持每轮固定模型数以维持效率，同时增加迭代间多样性以提高迁移性。在ImageNet上的实验表明，当从20个可用模型中动态选择4个时，SEA在相同效率下比现有方法迁移性高出8.5%，其优势还扩展到商业视觉API和大型视觉语言模型等实际系统。</div>
</details>
</div>
<div class="card">
<div class="title">Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation</div>
<div class="meta-line">Authors: Lingfeng Zhang, Yuchen Zhang, Hongsheng Li, Haoxiang Fu, Yingbo Tang, Hangjun Ye, Long Chen, Xiaojun Liang, Xiaoshuai Hao, Wenbo Ding</div>
<div class="meta-line">First: 2025-11-17T11:39:20+00:00 · Latest: 2025-11-17T11:39:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13269v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13269v1">PDF</a> · <a href="https://github.com/linglingxiansen/SpatialSKy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>您的视觉语言模型是否具备空间智能？面向无人机导航的综合空间智能基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型凭借其强大的视觉感知与推理能力，已在无人机任务中得到广泛应用。然而现有视觉语言模型在无人机场景中的空间智能能力仍待深入探索，其动态环境导航与解析的有效性引发关注。为填补这一空白，我们推出SpatialSky-Bench——专为评估无人机导航中视觉语言模型空间智能能力构建的综合基准。该基准包含环境感知与场景理解两大类别，细分为边界框、颜色、距离、高度及着陆安全分析等13个子类。对主流开源与闭源视觉语言模型的大规模评估显示，其在复杂无人机导航场景中的表现不尽如人意，凸显出空间能力存在显著不足。为此，我们开发了包含百万样本、涵盖多场景多样化标注的SpatialSky-Dataset数据集，并基于此推出专精于多粒度多语境无人机空间推理的Sky-VLM模型。大量实验结果表明，Sky-VLM在所有基准任务中均达到最先进性能，为开发适用于无人机场景的视觉语言模型开辟了新途径。源代码详见：https://github.com/linglingxiansen/SpatialSKy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are increasingly used in UAV navigation, but their spatial intelligence in such dynamic environments remains unverified, motivating the creation of SpatialSky-Bench to systematically assess these capabilities. The benchmark evaluates VLMs across 13 subcategories like distance estimation and landing safety, revealing poor performance in complex scenarios, which led to the development of Sky-VLM, a specialized model trained on the SpatialSky-Dataset with 1M annotated samples. Experiments show that Sky-VLM achieves state-of-the-art results across all benchmark tasks, demonstrating its effectiveness in enhancing spatial reasoning for UAV applications.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在无人机导航中空间智能能力未被充分探索的问题，旨在解决其在动态空中环境中的有效性担忧。作者提出了包含环境感知和场景理解两大类别、13个子类别的SpatialSky-Bench基准，并利用包含100万标注样本的SpatialSky-Dataset开发了专用于无人机空间推理的Sky-VLM。实验结果表明主流视觉语言模型在复杂无人机场景中表现不佳，而Sky-VLM在所有基准任务中均达到最先进性能，证明了其在无人机应用中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges</div>
<div class="meta-line">Authors: Junlong Li, Huaiyuan Xu, Sijie Cheng, Kejun Wu, Kim-Hui Yap, Lap-Pui Chau, Yi Wang</div>
<div class="meta-line">First: 2025-11-17T11:21:42+00:00 · Latest: 2025-11-17T11:21:42+00:00</div>
<div class="meta-line">Comments: 26 pages, 8 figures, 8 tables, Under peer-review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13261v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13261v1">PDF</a> · <a href="https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>构建以自我为中心的程序化AI助手：方法、基准与挑战</div>
<div class="mono" style="margin-top:8px">受视觉语言模型（VLM）与自我中心感知研究最新进展的推动，我们提出了一种以第一人称视角逐步支持日常程序化任务的自我中心程序化AI助手（EgoProceAssist）概念。本研究首先明确了三大核心任务：自我中心程序错误检测、自我中心程序学习及自我中心程序问答，通过新分类法界定了EgoProceAssist的核心功能。具体而言，我们系统梳理了当前三大领域的技术方法、相关数据集与评估指标。为揭示所提EgoProceAssist与现有基于VLM的AI助手之间的差距，我们设计了创新实验并对代表性VLM方法进行全面评估。基于研究发现与技术分析，我们探讨了未来挑战并提出了研究方向。本研究的完整资源清单已发布于持续更新的动态资源库：https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by advances in vision language models and egocentric perception, this study introduces an egocentric procedural AI assistant designed to support daily step-by-step tasks from a first-person view. The method involves defining three core tasks—procedural error detection, procedural learning, and procedural question answering—within a new taxonomy, reviewing current techniques, datasets, and metrics, and evaluating representative VLM-based methods through novel experiments. Key experimental results reveal significant gaps between the proposed assistant and existing VLM-based approaches, highlighting challenges and suggesting future research directions.</div>
<div class="mono" style="margin-top:8px">本研究受视觉语言模型和第一人称感知进展的驱动，提出了一种以第一人称视角支持日常分步任务的自我中心程序化AI助手。方法包括定义错误检测、程序学习和问答三个核心任务，并全面综述相关技术、数据集和评估指标，同时通过新实验评估代表性VLM方法。实验结果表明当前VLM在这些任务上存在明显性能差距，揭示了关键挑战并提出了未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</div>
<div class="meta-line">Authors: Ao Li, Yuxiang Duan, Jinghui Zhang, Congbo Ma, Yutong Xie, Gustavo Carneiro, Mohammad Yaqub, Hu Wang</div>
<div class="meta-line">First: 2025-07-28T08:44:58+00:00 · Latest: 2025-11-17T11:15:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20630v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20630v2">PDF</a> · <a href="https://github.com/liaolea/TransPrune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at https://github.com/liaolea/TransPrune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TransPrune：面向高效大型视觉语言模型的令牌转移剪枝</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型虽推动了多模态学习发展，但因视觉令牌数量庞大导致计算成本高昂，亟需通过令牌剪枝提升推理效率。现有方法多依赖注意力机制评估令牌重要性，但存在位置偏差等固有局限。本研究从令牌表征转移的新视角出发，发现其变化能有效反映语义信息。基于此提出TransPrune——一种无需训练的高效令牌剪枝方法，通过综合评估令牌转移变异度（衡量表征模长与方向的变化）和指令引导注意力（衡量指令对图像令牌的关注强度）来渐进剪枝。在八个基准测试中，TransPrune在LLaVA-v1.5等模型上保持与原模型相当的多模态性能，同时将推理TFLOPs降低超50%。值得注意的是，仅凭令牌转移变异度即可达到与注意力方法相当的效果，无需依赖注意力机制。代码将在论文录用后发布于https://github.com/liaolea/TransPrune。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) face high computational costs from numerous visual tokens, motivating token pruning to improve efficiency. The proposed TransPrune method assesses token importance by combining Token Transition Variation (measuring changes in token representation magnitude and direction) and Instruction-Guided Attention, enabling training-free progressive pruning. Experiments show TransPrune maintains comparable performance to original LVLMs like LLaVA-v1.5 across eight benchmarks while reducing inference TFLOPs by over half, with Token Transition Variation alone matching attention-based methods.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型因大量视觉令牌导致高计算成本，需通过令牌剪枝提升效率。TransPrune方法结合令牌转移变化（TTV）和指令引导注意力（IGA）评估令牌重要性，TTV衡量令牌表示的幅度和方向变化，IGA评估指令对图像令牌的关注强度，实现无需训练的剪枝。实验表明，TransPrune在八个基准测试中保持与LLaVA-v1.5和LLaVA-Next相当的性能，同时将推理TFLOPs减少一半以上，且仅用TTV即可达到基于注意力方法的水平。</div>
</details>
</div>
<div class="card">
<div class="title">MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</div>
<div class="meta-line">Authors: Malek Al Abed, Sebiha Demir, Anne Groteklaes, Elodie Germani, Shahrooz Faghihroohi, Hemmen Sabir, Shadi Albarqouni</div>
<div class="meta-line">First: 2025-11-17T10:51:11+00:00 · Latest: 2025-11-17T10:51:11+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13232v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MRIQT：面向新生儿超低场磁共振成像质量迁移的物理感知扩散模型</div>
<div class="mono" style="margin-top:8px">便携式超低场磁共振成像（uLF-MRI，0.064 T）为新生儿护理提供了便捷的神经影像技术，但与高场强（HF）MRI相比存在低信噪比和诊断质量较差的问题。我们提出MRIQT——一个用于实现uLF到HF MRI图像质量迁移的三维条件扩散框架。该框架融合了物理一致的uLF模拟现实K空间退化技术、采用无分类器引导的v预测实现稳定图像生成，以及基于信噪比加权的三维感知损失以保障解剖保真度。模型通过卷积分注意力UNet架构，在相同扫描条件下对含噪uLF输入进行去噪处理，实现结构保持的影像转换。基于包含多种病理类型的新生儿队列训练，MRIQT在PSNR指标上以15.3%的优势超越现有GAN与CNN基线模型，较最优技术水平提升1.78%，且85%的输出结果被医师评定为质量良好且病理特征清晰。MRIQT实现了基于扩散模型的便携式超低场MRI高保真增强，为新生儿脑部评估提供可靠支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of low signal-to-noise ratio and poor diagnostic quality in portable ultra-low-field MRI (uLF-MRI) for neonatal care compared to high-field MRI. The authors propose MRIQT, a 3D conditional diffusion framework that employs physics-aware K-space degradation for realistic uLF simulation, v-prediction with classifier-free guidance for stable generation, and an SNR-weighted 3D perceptual loss to preserve anatomical details. Experimental results on a neonatal cohort with diverse pathologies show that MRIQT outperforms recent GAN and CNN baselines by 15.3% in PSNR and achieves 1.78% improvement over state-of-the-art methods, with physicians rating 85% of its outputs as good quality with clear pathology visibility.</div>
<div class="mono" style="margin-top:8px">便携式超低场MRI为新生儿神经成像提供了便利，但其信噪比低且诊断质量较差。为此，MRIQT提出了一种三维条件扩散框架，结合K空间退化实现物理一致模拟、采用v预测与无分类器引导，并通过信噪比加权的感知损失和体积注意力UNet架构保持解剖结构。在新生儿队列实验中，MRIQT在PSNR上超越GAN和CNN基线15.3%，85%的输出被医生评为质量良好且病理清晰可见。</div>
</details>
</div>
<div class="card">
<div class="title">On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning</div>
<div class="meta-line">Authors: Simon Kurz, Jian-Jia Chen, Lucie Flek, Zhixue Zhao</div>
<div class="meta-line">First: 2024-08-26T16:29:13+00:00 · Latest: 2025-11-17T10:48:59+00:00</div>
<div class="meta-line">Comments: Accepted for publication in TACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.14398v4">Abs</a> · <a href="https://arxiv.org/pdf/2408.14398v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论语言定向剪枝的局限性：探究多语言大语言模型剪枝中的校准语言影响</div>
<div class="mono" style="margin-top:8px">大语言模型剪枝技术近期在训练后压缩和无须重新训练的场景中取得尖端成果，同时保持高预测性能。然而现有研究主要基于英语文本进行校准，忽视了现代大语言模型的多语言特性及其在非英语场景的频繁应用。本文通过深度分析多语言模型在单语应用剪枝后的性能与内部表征变化，首次系统比较了不同校准语言在多语言场景下的剪枝效果，涵盖多样语言、任务、模型及前沿剪枝技术。我们进一步解析剪枝模型的潜在子空间、剪枝掩码及独立神经元，发现目标语言校准虽能有效维持困惑度并产生高信噪比，但未必持续提升下游任务性能。对三个层级内部表征的深入分析揭示了当前剪枝方法的根本局限：虽然能有效保留语言特异性等主导信息，但不足以抵消对知识存储与推理至关重要的语言无关细微特征的损失。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limitations of language-targeted pruning in multilingual large language models, motivated by the observation that prior pruning methods primarily rely on English calibration data despite the models&#x27; multilingual capabilities. The authors conduct a comprehensive empirical analysis comparing different calibration languages across various languages, tasks, models, and state-of-the-art pruning techniques, while examining latent subspaces, pruning masks, and individual neurons. Experimental results show that while target-language calibration effectively maintains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance, revealing that current pruning approaches preserve dominant language-specific features but fail to protect nuanced language-agnostic features essential for knowledge retention and reasoning.</div>
<div class="mono" style="margin-top:8px">本研究探讨多语言大模型中语言定向剪枝的局限性，其动机在于现有剪枝方法主要依赖英语校准，忽视了模型的多语言特性。作者通过比较不同校准语言在多种任务、模型和先进剪枝技术中的表现，并对潜在子空间、剪枝掩码及单个神经元进行分析，开展了全面的实证研究。实验结果表明，尽管目标语言校准能有效保持困惑度并产生高信噪比，但无法持续提升下游任务性能，这揭示了当前剪枝方法虽能保留主导的语言特定特征，却损失了对知识保持和推理至关重要的语言无关细微特征。</div>
</details>
</div>
<div class="card">
<div class="title">DeToNATION: Decoupled Torch Network-Aware Training on Interlinked Online Nodes</div>
<div class="meta-line">Authors: Mogens Henrik From, Jacob Nielsen, Lukas Galke Poech, Peter Schneider-Kamp</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-10T17:55:59+00:00 · Latest: 2025-11-17T10:34:58+00:00</div>
<div class="meta-line">Comments: Accepted as a paper at AAAI 2026 Main Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.06728v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.06728v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, DeMo assumes that models fit on a single accelerator. We relax this assumption and introduce FlexDeMo, whereby nodes fully shard model parameters locally between different accelerators, while inter-node communication is reduced by synchronizing only fast-moving components instead of the full gradients -- resulting in a hybrid sharded data parallel training strategy. We further introduce a framework, denoted as DeToNATION, that generalizes DeMo, FlexDeMo, and other popular distributed training schemes such as DiLoCo -- introducing new variations of replication schemes and challenging choices made in DeMo. Our results across language and vision domains show that FlexDeMo attains similar validation loss as hybrid sharded data parallel training employing AdamW and full gradient synchronization, while being substantially faster. FlexDeMo is thus a promising distributed training scheme for the largest machine learning models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeToNATION：互联在线节点上的解耦式火炬网络感知训练</div>
<div class="mono" style="margin-top:8px">训练大型神经网络模型需要大量计算资源，通常分布在多个节点和加速器上。最新研究表明仅交换梯度的快速移动分量并在本地累积动量可能已足够。然而解耦动量假设模型可适配单加速器。我们放宽该假设提出FlexDeMo方案：节点在本地不同加速器间完全分片模型参数，同时通过仅同步快速移动分量替代完整梯度来减少节点间通信，形成混合分片数据并行训练策略。我们进一步提出DeToNATION框架，该框架泛化解耦动量、FlexDeMo及DiLoCo等流行分布式训练方案，引入新的复制策略变体并挑战解耦动量中的关键选择。在语言与视觉领域的实验表明，FlexDeMo在达到与采用AdamW及全梯度同步的混合分片数据并行训练相近验证损失的同时，训练速度显著提升。因此FlexDeMo为超大规模机器学习模型提供了极具前景的分布式训练方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training large neural networks requires distributed computation across multiple nodes, but existing methods like Decoupled Momentum (DeMo) assume models fit on a single accelerator. To address this, FlexDeMo is introduced, which shards model parameters locally across accelerators and reduces inter-node communication by synchronizing only fast-moving gradient components instead of full gradients, forming a hybrid sharded data parallel strategy. Experimental results on language and vision tasks show that FlexDeMo achieves comparable validation loss to full gradient synchronization methods like AdamW while significantly improving training speed, making it a promising approach for large-scale model training.</div>
<div class="mono" style="margin-top:8px">训练大型神经网络需要分布式计算，但现有方法如解耦动量假设模型可置于单个加速器上。本研究提出FlexDeMo，在本地加速器间分片模型参数，同时通过仅同步快速变化的梯度分量而非完整梯度来减少节点间通信，形成混合分片数据并行方法。在语言和视觉任务上的实验表明，FlexDeMo在达到与完整梯度同步方法相当验证损失的同时，显著提升了训练速度，为大规模模型扩展提供了可行方案。</div>
</details>
</div>
<div class="card">
<div class="title">3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation</div>
<div class="meta-line">Authors: Seonho Lee, Jiho Choi, Inha Kang, Jiwook Kim, Junsung Park, Hyunjung Shim</div>
<div class="meta-line">First: 2025-06-11T15:56:59+00:00 · Latest: 2025-11-17T10:19:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09883v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.09883v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown remarkable performance on diverse visual and linguistic tasks, yet they remain fundamentally limited in their understanding of 3D spatial structures. We propose Geometric Distillation, a lightweight, annotation-free fine-tuning framework that injects human-inspired geometric cues into pretrained VLMs without modifying their architecture. By distilling (1) sparse correspondences, (2) relative depth relations, and (3) dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R, VGGT), our method shapes representations to be geometry-aware while remaining compatible with natural image-text inputs. Through extensive evaluations on 3D vision-language reasoning and 3D perception benchmarks, our method consistently outperforms prior approaches, achieving improved 3D spatial reasoning with significantly lower computational cost. Our work demonstrates a scalable and efficient path to bridge 2D-trained VLMs with 3D understanding, opening up wider use in spatially grounded multimodal tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何蒸馏的3D感知视觉语言模型微调</div>
<div class="mono" style="margin-top:8px">视觉语言模型在多样化的视觉与语言任务中表现出卓越性能，但其对三维空间结构的理解仍存在根本性局限。我们提出几何蒸馏——一种轻量级、无需标注的微调框架，可在不改变预训练模型架构的前提下，将类人几何线索注入视觉语言模型。通过从现成的三维基础模型蒸馏（1）稀疏对应关系（2）相对深度关系（3）密集代价体积，我们的方法能塑造具备几何感知的表示，同时保持对自然图像文本输入的兼容性。在三维视觉语言推理与感知基准测试中，本方法持续超越现有方案，以显著降低的计算成本实现更强的三维空间推理能力。这项工作展示了将二维训练的视觉语言模型与三维理解相连接的可扩展高效路径，为空间基础多模态任务开辟了更广阔的应用前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models excel in 2D tasks but lack 3D spatial understanding, motivating the development of a method to enhance their geometric reasoning without architectural changes. The proposed Geometric Distillation framework fine-tunes pretrained VLMs by distilling sparse correspondences, relative depth relations, and dense cost volumes from existing 3D foundation models, enabling geometry-aware representations from standard image-text inputs. Experimental results on 3D vision-language reasoning and perception benchmarks show consistent performance improvements over prior methods, achieving better 3D spatial reasoning with reduced computational costs.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在三维空间结构理解上的局限性，提出了几何蒸馏这一轻量级微调框架，无需修改模型架构即可注入几何线索。该方法从现成的三维基础模型中蒸馏稀疏对应、相对深度关系和密集代价体积，以构建兼容自然图像-文本输入的几何感知表示。在三维视觉语言推理和感知基准测试中，该方法持续优于先前方法，以显著降低的计算成本实现了更强的三维空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Video Spatial Reasoning with Object-Centric 3D Rollout</div>
<div class="meta-line">Authors: Haoran Tang, Meng Cao, Ruyang Liu, Xiaoxi Liang, Linglong Li, Ge Li, Xiaodan Liang</div>
<div class="meta-line">First: 2025-11-17T09:53:41+00:00 · Latest: 2025-11-17T09:53:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13190v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13190v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR&#x27;s superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视频空间推理的以物体为中心的三维展开方法</div>
<div class="mono" style="margin-top:8px">多模态大语言模型近期在视觉语言理解方面展现出卓越能力，但实现稳健的视频空间推理——即理解动态三维场景中物体位置、朝向及相互关系的能力——仍是待解决的关键挑战。现有方法主要依赖空间基础监督微调或强化学习，但我们发现这类模型常出现查询锁定的推理模式，仅关注提示中明确提及的物体而忽略关键上下文线索。为此，我们提出以物体为中心的三维展开策略，通过在训练中对选定物体的三维几何结构引入结构化扰动，通过弱化物体特定视觉线索并将变换后的几何结构投影至二维空间，迫使模型进行全场景整体推理。我们进一步设计了基于展开的训练流程，联合利用原始视频与区域噪声视频以优化空间推理轨迹。实验表明取得最先进性能：我们的30亿参数模型在VSI-Bench上达到47.5%准确率，超越多个70亿基线模型。消融实验证实本方法优于先前的展开策略（如T-GRPO、NoisyRollout）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling robust video spatial reasoning in Multi-modal Large Language Models, which often exhibit query-locked reasoning by focusing only on explicitly mentioned objects while ignoring contextual cues. The proposed method, Object-Centric 3D Rollout (OCR), introduces structured perturbations to the 3D geometry of selected objects during training, degrading object-specific visual cues and projecting altered geometry into 2D space to compel holistic scene reasoning, combined with a rollout-based training pipeline using both vanilla and region-noisy videos. Experimental results show state-of-the-art performance, with a 3B-parameter model achieving 47.5% accuracy on VSI-Bench, outperforming several 7B baselines and demonstrating superiority over prior rollout strategies in ablations.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态大语言模型在视频空间推理中的挑战，即模型常局限于查询中明确提及的对象而忽略上下文线索的问题，提出了一种对象中心三维展开方法。该方法通过在训练中对选定对象的三维几何结构引入结构化扰动，削弱对象特定视觉线索并将改变后的几何投影至二维空间，迫使模型进行整体场景推理，并结合使用原始视频和区域噪声视频的展开式训练流程。实验结果表明，所提出的3B参数模型在VSI-Bench上达到47.5%的准确率，优于多个7B基线模型，消融研究证实了该方法相对于先前展开策略的优越性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251118_1147.html">20251118_1147</a>
<a href="archive/20251118_1028.html">20251118_1028</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
